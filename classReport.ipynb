{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、分布式集群及编程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hadoop集群环境搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce编程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HBase及数据库操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spack集群环境搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spack SQL学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数学基础及数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.信息墒和交叉墒\n",
    "**信息墒公式**\n",
    "\n",
    "$$ H(X) =  -\\sum_{i=1}^n{p(x_i)} log p(x_i) $$\n",
    "\n",
    "其中$$P(x_i)代表随机事件, X为x_i的概率$$\n",
    "\n",
    "\n",
    "**<font color=#00BFFF>信息量</font>:*信息的大小跟随机事件的概率有关。越小概率的事情发生了产生的信息量越大，越大概率的事情发生了产生的信息量越小***\n",
    "\n",
    "**<font color=#00BFFF>信息墒</font>:*信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即*** \n",
    "\n",
    "$$ H(X) =  -sum(p(x) log_2 p(x_i)) $$\n",
    "**转换一下**\n",
    "$$ H(X) =  -\\sum_{i=1}^n{p(x_i)} log p(x_i) $$\n",
    "\n",
    "**<font color=#00BFFF>补充</font>:这里再说一个对信息熵的理解。信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为1种情况，那么对应概率为1，那么对应的信息熵为0），此时的信息熵较小**\n",
    "\n",
    "**<font color=#00BFFF>交叉墒</font>:*熟悉机器学习的人都知道分类模型中会使用交叉熵作损失函数，也一定对吴恩达的机器学习视频中猫分类器使用的二分类交叉熵印象深刻，但交叉熵究竟是什么？字面上看，交叉熵分两部分“<font color=red>交叉</font>”和“<font color=red>墒</font>”***\n",
    "\n",
    "**<font color=#00BFFF>墒</font>:*熵是服从某一特定概率分布事件的理论最小平均编码长度”，只要我们知道了任何事件的概率分布，我们就可以计算它的熵***\n",
    "\n",
    "**<font color=#00BFFF>交叉墒$\\geqslant$墒</font>:*交叉熵使用$H(P,Q)$表示，意味着使用$P$计算期望，使用$Q$计算编码长度；所以$H(P,Q)$并不一定等于$H(Q,P)$，除了在$P=Q$的情况下，$H(P,Q) = H(Q,P) = H(P),其中$***\n",
    "$$H(P,Q)=E_x\\widetilde~_p[-logQ(x)]$$\n",
    "\n",
    "**<font color=#00BFFF>有一点很微妙但很重要</font>:*对于期望，我们使用真实概率分布$P$来计算；对于编码长度，我们使用假设的概率分布$Q$来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。换句话说，如果我们的估计是完美的，即$Q=P$，那么有$H(P,Q) = H(P)$，否则$H(P,Q) > H(P)$***\n",
    "\n",
    "**<font color=#00BFFF>二分类交叉墒</font>:*在二分类模型中，标签只有是和否两种；这时，可以使用二分类交叉熵作为损失函数。假设数据集中只有猫和狗的照片，则交叉熵公式中只包含两种可能性:***\n",
    "\n",
    "$$ H(P,Q) = -\\sum_{i=(cat,dog)}{P(i)}{logQ(i)} $$\n",
    "$$=  -P(cat)logQ(cat)~-~P(dog)logQ(dog)$$\n",
    "**又因为**\n",
    "$$P(cat)=1~-~P(dog)$$\n",
    "**所以交叉墒可以表示为**\n",
    "$$H(P,Q)=-P(cat)logQ(cat)~-~(1~-~P(dog))log(1~-~P(dog))$$\n",
    "**使用如下定义:**\n",
    "$$P = P(cat)$$\n",
    "$$Q = P(cat)$$\n",
    "**二分类的交叉熵可以写作如下形式，看起来就熟悉多了**\n",
    "$$BinaryCrossEtropy=-PlogP-(1-\\tilde{P})log(1-\\tilde{P})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.假设校验\n",
    "**<font color=#00BFFF>假设检验定义</font>:*假设检验是先对总体参数提出一个假设值，然后利用样本信息判断这一假设是否成立***\n",
    "\n",
    "**<font color=#00BFFF>假设检验的假设</font>:*由定义可知，我们需要对结果进行假设，然后拿样本数据去验证这个假设。所以做假设检验时会设置两个假设：***\n",
    "- 一种叫<font color=#00BFFF>原假设</font>:$~$也叫零假设，用$H_0$表示。原假设一般是统计者想要拒绝的假设。原假设的设置一般为：等于=、大于等于$\\geq$、小于等于$\\leq$。\n",
    "\n",
    "- 另外一种叫<font color=#00BFFF>设备假设</font>:$~$，用$H_1$表示。备则假设是统计者想要接受的假设。备择假设的设置一般为：不等于$\\neq$、大于>、小于<。\n",
    "\n",
    "**<font color=#00BFFF>弃真错误、取伪错误</font>:*我们通过样本数据来判断总体参数的假设是否成立，但样本时随机的，因而有可能出现小概率的错误。这种错误分两种，一种是弃真错误，另一种是取伪错误***\n",
    "\n",
    ">- <font color=#00BFFF>弃真错误</font>:$~$也叫第I类错误或α错误，它是指原假设实际上是真的，但通过样本估计总体后，拒绝了原假设。明显这是错误的，我们拒绝了真实的原假设，所以叫弃真错误，这个错误的概率我们记为α。这个值也是显著性水平，在假设检验之前我们会规定这个概率的大小\n",
    "\n",
    ">- <font color=#00BFFF>取伪错误</font>:$~$也叫第II类错误或β错误，它是指 原假设实际上假的，但通过样本估计总体后，接受了原假设。明显者是错误的，我们接受的原假设实际上是假的，所以叫取伪错误，这个错误的概率我们记为β\n",
    "\n",
    "**<font color=#00BFFF>显著性水平</font>:*显著性水平是指当原假设实际上正确时，检验统计量落在拒绝域的概率，简单理解就是放弃真错误的概率。***\n",
    "- 显著性水平α越小，犯第I类错误的概率自然越小，一般取值：0.01、0.05、0.1等\n",
    "\n",
    "- 当给定了检验的显著水平a=0.05时，进行双侧检验的Z值为1.96，t值为 。\n",
    "\n",
    "- 当给定了检验的显著水平a=0.01时，进行双侧检验的Z值为2.58 。\n",
    "\n",
    "- 当给定了检验的显著水平a=0.05时，进行单侧检验的Z值为1.645 。\n",
    "\n",
    "- 当给定了检验的显著水平a=0.01时，进行单侧检验的Z值为2.33\n",
    "\n",
    "**<font color=#00BFFF>检验方式</font>:*检验方式分为两种：双侧检验和单侧检验。***\n",
    "> <font color=#00BFFF>单侧检验</font>又分为两种：左侧检验和右侧检验。备择假设带有特定的方向性 形式为\">\"\"<\"的假设检验，称为单侧检验 \n",
    "- \"<\"称为左侧检验\n",
    "- \">\"称为右侧检验\n",
    " \n",
    "> <font color=#00BFFF>双侧检验</font>：备择假设没有特定的方向性，形式为“≠”这种检验假设称为双侧检验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#00BFFF>3.1 假设检验定义</font>:*先对总体参数提出某种假设，然后利用样本数据判断假设是否成立。在逻辑上，假设检验采用了反证法，即先提出假设，再通过适当的统计学方法证明这个假设基本不可能是真的***\n",
    "\n",
    "**<font color=#00BFFF>3.2 假设检验的术语</font>:*先对总体参数提出某种假设，然后利用样本数据判断假设是否成立。在逻辑上，假设检验采用了反证法，即先提出假设，再通过适当的统计学方法证明这个假设基本不可能是真的***\n",
    ">- <font color=#00BFFF>3.2.1零假设:$~$</font>是试验者想收集证据予以反对的假设，也称为原假设，通常记为 $H_0$。*<font color=gray>例如：零假设是测试版本的指标均值小于等于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.2备择假设:$~$</font>是试验者想收集证据予以支持的假设，通常记为$H_1或H_a$。*<font color=gray>例如：备择假设是测试版本的指标均值大于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.3双侧检验:$~$</font>如果备择假设没有特定的方向性，并含有符号“≠”，这样的检验称为双尾检验。*<font color=gray>例如：零假设是测试版本的指标均值等于原始版本的指标均值，备择假设是测试版本的指标均值不等于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.4单侧检验:$~$</font>如果备择假设具有特定的方向性，并含有符号 “>” 或 “<” ，这样的检验称为单尾检验。单尾检验分为左尾和右尾。*<font color=gray>例如：零假设是测试版本的指标均值小于等于原始版本的指标均值，备择假设是测试版本的指标均值大于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.5检验统计量:$~$</font>用于假设检验计算的统计量。*<font color=gray>例如：Z值、t值、F值、卡方值</font>*\n",
    ">- <font color=#00BFFF>3.2.6显著性水平:$~$</font>当零假设为真时，错误拒绝零假设的临界概率，即犯第一类错误的最大概率，用α表示。*<font color=gray>例如：在5%的显著性水平下，样本数据拒绝原假设。</font>*\n",
    ">- <font color=#00BFFF>3.2.7置信度:$~$</font>置信区间包含总体参数的确信程度，即1-α。*<font color=gray>例如：95%的置信度表明有95%的确信度相信置信区间包含总体参数（假设进行100次抽样，有95次计算出的置信区间包含总体参数）。</font>*\n",
    ">- <font color=#00BFFF>3.2.8置信区间:$~$</font>包含总体参数的随机区间。\n",
    ">- <font color=#00BFFF>3.2.9功效:$~$</font>正确拒绝零假设的概率，即1-β。当检验结果是不能拒绝零假设，人们又需要进行决策时，需要关注功效。功效越大，犯第二类错误的可能性越小。\n",
    ">- <font color=#00BFFF>3.2.10临界值:$~$</font>与检验统计量的具体值进行比较的值。是在概率密度分布图上的分位数。这个分位数在实际计算中比较麻烦，它需要对数据分布的密度函数积分来获得。\n",
    ">- <font color=#00BFFF>3.2.11临界区域:$~$</font>拒绝原假设的检验统计量的取值范围，也称为拒绝域，是由一组临界值组成的区域。如果检验统计量在拒绝域内，那么我们拒绝原假设。\n",
    ">- <font color=#00BFFF>3.2.12P值:$~$</font>在零假设为真时所得到的样本观察结果或获得更极端结果的概率。也可以说，p值是当原假设为真时，错误拒绝原假设的实际概率。\n",
    ">- <font color=#00BFFF>3.2.13效应量:$~$</font>样本间差异或相关程度的量化指标。效应量越大，两组平均数离得越远，差异越大。如果结果具有统计显著性，那么有必要报告效应量的大小。效应量太小，意味着即使结果有统计显著性，也缺乏实用价值。\n",
    "\n",
    "**<font color=#00BFFF>3.3 假设检验的两类错误</font>:**\n",
    "> - <font color=#00BFFF>3.3.1第 I 类错误(弃真错误):$~$</font>零假设为真时错误地拒绝了零假设。犯第 I 类错误的最大概率记为 α（alpha）。\n",
    ">  - <font color=#00BFFF>3.3.1第 II 类错误(取伪错误):$~$</font> 零假设为假时错误地接受了零假设。犯第 II 类错误的最大概率记为 β（beta）。\n",
    "\n",
    "![alt text](jiashe.png)\n",
    "\n",
    "**<font color=#00BFFF>注意</font>:**\n",
    "> - 在假设检验中，我们可能在决策上犯这两类错误。一般来说，在样本量确定的情况下，任何决策无法同时避免这两类错误的发生，即在减少第一类错误发生的同时，会增大第二类错误发生的几率，或者在减少第二类错误发生的同时，会增大第一类错误发生的几率\n",
    "> - 在大多数情况下，人们会控制第一类错误发生的概率。在进行假设检验时，人们通过事先给定显著性水平α的值来控制第一类错误发生的概率，常用的 α 值有 0.01，0.05，0.1。如果犯第一类错误的成本不高，那么可以选择较大的α值；如果犯第一类错误的成本很高，则选择较小的α值。\n",
    "\n",
    "**<font color=#00BFFF>3.4 假设检验的步骤</font>:**\n",
    "> 1. 定义总体\n",
    "> 2. 确定原假设和备择假设\n",
    "> 3. 选择检验统计量（确定假设检验的种类）\n",
    "> 4. 选择显著性水平\n",
    "> 5. 从总体进行抽样，得到一定的数据\n",
    "> 6. 根据样本数据计算检验统计量的具体值\n",
    "> 7. 依据所构造的检验统计量的抽样分布和显著性水平，确定临界值和拒绝域\n",
    "> 8. 比较检验统计量的值与临界值，如果检验统计量的值在拒绝域内，则拒绝原假设\n",
    "\n",
    "**<font color=#00BFFF>3.5 假设检验的决策标准</font>:*由于检验是利用事先给定显著性水平的方法来控制犯错概率的，所以对于两个数据比较相近的假设检验，我们无法知道哪一个假设更容易犯错，即我们通过这种方法只能知道根据这次抽样而犯第一类错误的最大概率，而无法知道具体在多大概率水平上犯错。计算P值有效的解决了这个问题，P值其实就是按照抽样分布计算的一个概率值，这个值是根据检验统计量计算出来的。通过直接比较P值与给定的显著性水平α的大小就可以知道是否拒绝原假设，显然这就可以代替比较检验统计量的具体值与临界值的大小的方法。***\n",
    "\n",
    "**<font color=#00BFFF>注意</font>**\n",
    "> - p值小于0.01---强有力的证据判定备择假设为真；\n",
    "> - p值介于0.01~0.05---有力的证据判定备择假设为真；\n",
    "> - p值介于0.05~0.1---较弱的证据判定备择假设为真；\n",
    "> - p值大于0.1---没有足够的证据判定备择假设为真。\n",
    "\n",
    "**<font color=#00BFFF>3.6 假设检验的种类</font>:$~$*Z检验、t检验、卡方检验、F检验。***\n",
    "> - <font color=#00BFFF>3.6.1 Z检验:$~$</font>Z检验用于比较样本和总体的均值是否不同或者两个样本的均值是否不同。检验统计量z值的分布服从正态分布,由于总体方差一般都是未知的，并且Z检验只适合大样本的情况。\n",
    "> - <font color=#00BFFF>3.6.2 T检验:$~$</font>事先不知道总体方差，另外，如果总体不服从正态分布，那么样本量要大于等于30，如果总体服从正态分布，那么对样本量没有要求.\n",
    "**<font color=#00BFFF>t检验分为单样本t检验，配对t检验和独立样本t检验。</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.矩阵计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.特征值及特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.使用matplotlib绘图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.主成因分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、机器学习及深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
