{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、分布式集群及编程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. hadoop集群环境搭建(10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MapReduce编程(10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. HBase及数据库操作(10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spack集群环境搭建(10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RDD操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Spack SQL学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数学基础及数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 概率分布\n",
    "**<font color=#00BFFF>1.均匀分布:</font>** \n",
    ">- 离散随机变量的均匀分布：假设 X 有 k 个取值：x1, x2, ..., xk 则均匀分布的概率密度函数为\n",
    "> $$p(X=x_i)=\t\\frac{1}{k},~i=1,2,3,4···,k$$\n",
    "\n",
    ">- 连续随机变量的均匀分布：假设 X 在 [a, b] 上均匀分布，则其概率密度函数为\n",
    "> $$ p(X=x)=\\left\\{\\begin{matrix}\n",
    " 0, ~~~~~~~~x\\notin[a,b]\\\\\n",
    " \\frac{1}{b-a} ~~~~~~~~x\\in[a,b]\n",
    "\\end{matrix}\n",
    "\\right. $$\n",
    "**<font color=#00BFFF>2.伯努利分布:</font>** 参数为 θ∈[0,1]，设随机变量 X ∈ {0,1}，则概率分布函数为:\n",
    "> $$p(X=x)=\\phi^x(1-\\phi)^{1-x},x\\in\\left\\{ \\begin{matrix}\n",
    "0, \\\\\n",
    "\\end{matrix}\n",
    "\\right.\n",
    "\\left.\\begin{matrix}\n",
    "1\n",
    "\\end{matrix}\n",
    "\\right\\}\n",
    "$$\n",
    "> 期望$$E[X]=\\phi$$\n",
    "> 方差$$Var[X]=\\phi(1-\\phi)$$\n",
    "\n",
    "\n",
    "**<font color=#00BFFF>3.二项分布:</font>**\n",
    "> 假设试验只有两种结果：成功的概率为 θ，失败的概率为 1-θ. 则二项分布描述了：独立重复地进行 n 次试验中，成功 x 次的概率$$p(X=x)=\t\\frac{n!}{x!(n-x)!}\\phi^x(1-\\phi)^{n-x}, ~~x\\in\\left\\{ \\begin{matrix}\n",
    "0,\n",
    "\\end{matrix}\n",
    "\\right.\n",
    "\\left.\\begin{matrix}\n",
    "1,···,n\\\\\n",
    "\\end{matrix}\n",
    "\\right\\}$$\n",
    "> 期望$$E[X]=n\\phi$$\n",
    "> 方差$$Var[X]=n\\phi(1-\\phi)$$\n",
    "\n",
    "**<font color=#00BFFF>4.高斯分布:</font>**\n",
    "正态分布是很多应用中的合理选择。如果某个随机变量取值范围是实数，且对它的概率分布一无所知，通常会假设它服从正态分布。\n",
    ">- 建模的任务的真实分布通常都确实接近正态分布。中心极限定理表明，多个独立随机变量的和近似正态分布\n",
    ">- 在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）<br>\n",
    "\n",
    "> 典型的一维正态分布的概率密度函数为$$p(x)=\t\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x-\\mu)^2/(2\\sigma^2)},~~~\\infty <x<\\infty$$\n",
    "![jisuan](cacu.png)\n",
    "\n",
    "**<font color=#00BFFF>5.拉普拉斯分布:</font>**\n",
    "> 概率密度函数$$p(x;\\mu,\\gamma)=\\frac{1}{2\\gamma}exp(-\\frac{|x-\\mu|}{\\gamma})$$\n",
    "> 期望$$E[X]=\\mu$$\n",
    "> 方差$$Var[X]=2{\\gamma}^2$$\n",
    "![jisuan](cacu2.png)\n",
    "\n",
    "**<font color=#00BFFF>6.泊松分布:</font>** 假设已知事件在单位时间（或者单位面积）内发生的平均次数为 λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为 k 的概率\n",
    "> 概率密度函数$$p(X=k;\\lambda)=\\frac{e^{-\\lambda}\\lambda^k}{k!}$$\n",
    "> 期望$$E[X]=\\lambda$$\n",
    "> 方差$$Var[X]=\\lambda$$\n",
    "![jisuan](cacu3.png)\n",
    "\n",
    "**<font color=#00BFFF>7.指数分布:</font>** 若事件服从泊松分布，则该事件前后两次发生的时间间隔服从指数分布。由于时间间隔是个浮点数，因此指数分布是连续分布\n",
    "> 概率密度函数：（t为时间间隔）\n",
    ">$$ p(t;\\lambda)=\\left\\{\\begin{matrix}\n",
    " 0, ~~~~~~~~x<0\\\\\n",
    " \\frac{\\lambda}{exp(\\lambda t)} ~~~~~~~~t\\geq0\n",
    "\\end{matrix}\n",
    "\\right. $$\n",
    "\n",
    "> 期望$$E[t]=\\frac{1}{\\gamma}$$\n",
    "> 方差$$Var[t]=\\frac{1}{\\gamma^2}$$\n",
    "![jisuan](cacu4.png)\n",
    "\n",
    "**<font color=#00BFFF>8.伽马分布:</font>**\n",
    "> 若事件服从泊松分布，则事件第 i 次发生和第 i+k 次发生的时间间隔为伽玛分布。由于时间间隔是个浮点数，因此伽马分布是连续分布\n",
    "> 概率密度函数:\n",
    "> $$p(t;\\lambda,k)=\\frac{t^{k-1}\\lambda^ke^{(-\\lambda t)}}{\\Gamma(k)}$$\n",
    "> 其中,t为时间间隔,k称为形状参数,$\\lambda$称为尺度参数\n",
    "> 期望$$E[t]=\\frac{k}{\\lambda}$$\n",
    "> 方差$$Var[t]=\\frac{k}{\\lambda^2}$$\n",
    "![jisuan](cacu5.png)\n",
    "\n",
    "**<font color=#00BFFF>9.贝塔分布:</font>**\n",
    "> 贝塔分布是定义在 (0,1) 之间的连续概率分布\n",
    "> 如果随机变量 X 服从贝塔分布，则其概率密度函数为$$p(X,\\alpha,\\beta)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}X^{\\alpha-1}(1-X)^{\\beta-1}=\\frac{1}{B(\\alpha,\\beta)}X^{\\alpha-1}(1-X)^{\\beta-1}, 0\\leq X<1$$\n",
    "记做$$Beta(\\alpha,\\beta)$$\n",
    "> 期望$$E[t]=\\frac{\\alpha}{\\alpha+\\beta}$$\n",
    "> 方差$$Var[t]=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$$\n",
    "![jisuan](cacu6.png)\n",
    "\n",
    "**<font color=#00BFFF>10.多项式分布与狄里克雷分布:</font>**\n",
    "> 多项式分布的质量密度函数:$$Mult(m_1,m_2···m_k;\\vec\\mu,N)= \\frac{N!}{N_1!N_2!···N_k!}\t\\sum_{k=1}^{K}\\mu{_k}^{m_{k}}$$\n",
    "> 狄利克雷分布的概率密度函数:$$Dir(\\vec\\mu;\\vec\\alpha)=\\frac{\\Gamma(\t\\sum_{k=1}^{K}\\alpha_k)}{\\sum_{k=1}^{K}\\Gamma(\\alpha_k)}\\prod_{k=0}^K{\\mu_{k}^{\\alpha_k-1}}$$\n",
    ">- 多项式分布是针对离散型随机变量，通过求和获取概率\n",
    ">- 狄里克雷分布时针对连续型随机变量，通过求积分来获取概率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 信息墒和交叉墒\n",
    "**信息墒公式**\n",
    "\n",
    "$$ H(X) =  -\\sum_{i=1}^n{p(x_i)} log p(x_i) $$\n",
    "\n",
    "其中$$P(x_i)代表随机事件, X为x_i的概率$$\n",
    "\n",
    "\n",
    "**<font color=#00BFFF>信息量</font>:信息的大小跟随机事件的概率有关。越小概率的事情发生了产生的信息量越大，越大概率的事情发生了产生的信息量越小**\n",
    "\n",
    "**<font color=#00BFFF>信息墒</font>:信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即** \n",
    "\n",
    "$$ H(X) =  -sum(p(x) log_2 p(x_i)) $$\n",
    "**转换一下**\n",
    "$$ H(X) =  -\\sum_{i=1}^n{p(x_i)} log p(x_i) $$\n",
    "\n",
    "**<font color=#00BFFF>补充</font>:这里再说一个对信息熵的理解。信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为1种情况，那么对应概率为1，那么对应的信息熵为0），此时的信息熵较小**\n",
    "\n",
    "**<font color=#00BFFF>交叉墒</font>:*熟悉机器学习的人都知道分类模型中会使用交叉熵作损失函数，也一定对吴恩达的机器学习视频中猫分类器使用的二分类交叉熵印象深刻，但交叉熵究竟是什么？字面上看，交叉熵分两部分“<font color=red>交叉</font>”和“<font color=red>墒</font>”***\n",
    "\n",
    "**<font color=#00BFFF>墒</font>:*熵是服从某一特定概率分布事件的理论最小平均编码长度”，只要我们知道了任何事件的概率分布，我们就可以计算它的熵***\n",
    "\n",
    "**<font color=#00BFFF>交叉墒$\\geqslant$墒</font>:*交叉熵使用$H(P,Q)$表示，意味着使用$P$计算期望，使用$Q$计算编码长度；所以$H(P,Q)$并不一定等于$H(Q,P)$，除了在$P=Q$的情况下，$H(P,Q) = H(Q,P) = H(P),其中$***\n",
    "$$H(P,Q)=E_x\\widetilde~_p[-logQ(x)]$$\n",
    "\n",
    "**<font color=#00BFFF>有一点很微妙但很重要</font>:*对于期望，我们使用真实概率分布$P$来计算；对于编码长度，我们使用假设的概率分布$Q$来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。换句话说，如果我们的估计是完美的，即$Q=P$，那么有$H(P,Q) = H(P)$，否则$H(P,Q) > H(P)$***\n",
    "\n",
    "**<font color=#00BFFF>二分类交叉墒</font>:*在二分类模型中，标签只有是和否两种；这时，可以使用二分类交叉熵作为损失函数。假设数据集中只有猫和狗的照片，则交叉熵公式中只包含两种可能性:***\n",
    "\n",
    "$$ H(P,Q) = -\\sum_{i=(cat,dog)}{P(i)}{logQ(i)} $$\n",
    "$$=  -P(cat)logQ(cat)~-~P(dog)logQ(dog)$$\n",
    "**又因为**\n",
    "$$P(cat)=1~-~P(dog)$$\n",
    "**所以交叉墒可以表示为**\n",
    "$$H(P,Q)=-P(cat)logQ(cat)~-~(1~-~P(dog))log(1~-~P(dog))$$\n",
    "**使用如下定义:**\n",
    "$$P = P(cat)$$\n",
    "$$Q = P(cat)$$\n",
    "**二分类的交叉熵可以写作如下形式，看起来就熟悉多了**\n",
    "$$BinaryCrossEtropy=-PlogP-(1-\\tilde{P})log(1-\\tilde{P})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 假设校验\n",
    "**<font color=#00BFFF>3.1 假设检验定义</font>:*先对总体参数提出某种假设，然后利用样本数据判断假设是否成立。在逻辑上，假设检验采用了反证法，即先提出假设，再通过适当的统计学方法证明这个假设基本不可能是真的***\n",
    "\n",
    "**<font color=#00BFFF>3.2 假设检验的术语</font>:*先对总体参数提出某种假设，然后利用样本数据判断假设是否成立。在逻辑上，假设检验采用了反证法，即先提出假设，再通过适当的统计学方法证明这个假设基本不可能是真的***\n",
    ">- <font color=#00BFFF>3.2.1零假设:$~$</font>是试验者想收集证据予以反对的假设，也称为原假设，通常记为 $H_0$。*<font color=gray>例如：零假设是测试版本的指标均值小于等于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.2备择假设:$~$</font>是试验者想收集证据予以支持的假设，通常记为$H_1或H_a$。*<font color=gray>例如：备择假设是测试版本的指标均值大于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.3双侧检验:$~$</font>如果备择假设没有特定的方向性，并含有符号“≠”，这样的检验称为双尾检验。*<font color=gray>例如：零假设是测试版本的指标均值等于原始版本的指标均值，备择假设是测试版本的指标均值不等于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.4单侧检验:$~$</font>如果备择假设具有特定的方向性，并含有符号 “>” 或 “<” ，这样的检验称为单尾检验。单尾检验分为左尾和右尾。*<font color=gray>例如：零假设是测试版本的指标均值小于等于原始版本的指标均值，备择假设是测试版本的指标均值大于原始版本的指标均值。</font>*\n",
    ">- <font color=#00BFFF>3.2.5检验统计量:$~$</font>用于假设检验计算的统计量。*<font color=gray>例如：Z值、t值、F值、卡方值</font>*\n",
    ">- <font color=#00BFFF>3.2.6显著性水平:$~$</font>当零假设为真时，错误拒绝零假设的临界概率，即犯第一类错误的最大概率，用α表示。*<font color=gray>例如：在5%的显著性水平下，样本数据拒绝原假设。</font>*\n",
    ">- <font color=#00BFFF>3.2.7置信度:$~$</font>置信区间包含总体参数的确信程度，即1-α。*<font color=gray>例如：95%的置信度表明有95%的确信度相信置信区间包含总体参数（假设进行100次抽样，有95次计算出的置信区间包含总体参数）。</font>*\n",
    ">- <font color=#00BFFF>3.2.8置信区间:$~$</font>包含总体参数的随机区间。\n",
    ">- <font color=#00BFFF>3.2.9功效:$~$</font>正确拒绝零假设的概率，即1-β。当检验结果是不能拒绝零假设，人们又需要进行决策时，需要关注功效。功效越大，犯第二类错误的可能性越小。\n",
    ">- <font color=#00BFFF>3.2.10临界值:$~$</font>与检验统计量的具体值进行比较的值。是在概率密度分布图上的分位数。这个分位数在实际计算中比较麻烦，它需要对数据分布的密度函数积分来获得。\n",
    ">- <font color=#00BFFF>3.2.11临界区域:$~$</font>拒绝原假设的检验统计量的取值范围，也称为拒绝域，是由一组临界值组成的区域。如果检验统计量在拒绝域内，那么我们拒绝原假设。\n",
    ">- <font color=#00BFFF>3.2.12P值:$~$</font>在零假设为真时所得到的样本观察结果或获得更极端结果的概率。也可以说，p值是当原假设为真时，错误拒绝原假设的实际概率。\n",
    ">- <font color=#00BFFF>3.2.13效应量:$~$</font>样本间差异或相关程度的量化指标。效应量越大，两组平均数离得越远，差异越大。如果结果具有统计显著性，那么有必要报告效应量的大小。效应量太小，意味着即使结果有统计显著性，也缺乏实用价值。\n",
    "\n",
    "**<font color=#00BFFF>3.3 假设检验的两类错误</font>:**\n",
    "> - <font color=#00BFFF>3.3.1第 I 类错误(弃真错误):$~$</font>零假设为真时错误地拒绝了零假设。犯第 I 类错误的最大概率记为 α（alpha）。\n",
    ">  - <font color=#00BFFF>3.3.1第 II 类错误(取伪错误):$~$</font> 零假设为假时错误地接受了零假设。犯第 II 类错误的最大概率记为 β（beta）。\n",
    "\n",
    "![alt text](jiashe.png)\n",
    "\n",
    "**<font color=#00BFFF>注意</font>:**\n",
    "> - 在假设检验中，我们可能在决策上犯这两类错误。一般来说，在样本量确定的情况下，任何决策无法同时避免这两类错误的发生，即在减少第一类错误发生的同时，会增大第二类错误发生的几率，或者在减少第二类错误发生的同时，会增大第一类错误发生的几率\n",
    "> - 在大多数情况下，人们会控制第一类错误发生的概率。在进行假设检验时，人们通过事先给定显著性水平α的值来控制第一类错误发生的概率，常用的 α 值有 0.01，0.05，0.1。如果犯第一类错误的成本不高，那么可以选择较大的α值；如果犯第一类错误的成本很高，则选择较小的α值。\n",
    "\n",
    "**<font color=#00BFFF>3.4 假设检验的步骤</font>:**\n",
    "> 1. 定义总体\n",
    "> 2. 确定原假设和备择假设\n",
    "> 3. 选择检验统计量（确定假设检验的种类）\n",
    "> 4. 选择显著性水平\n",
    "> 5. 从总体进行抽样，得到一定的数据\n",
    "> 6. 根据样本数据计算检验统计量的具体值\n",
    "> 7. 依据所构造的检验统计量的抽样分布和显著性水平，确定临界值和拒绝域\n",
    "> 8. 比较检验统计量的值与临界值，如果检验统计量的值在拒绝域内，则拒绝原假设\n",
    "\n",
    "**<font color=#00BFFF>3.5 假设检验的决策标准</font>:*由于检验是利用事先给定显著性水平的方法来控制犯错概率的，所以对于两个数据比较相近的假设检验，我们无法知道哪一个假设更容易犯错，即我们通过这种方法只能知道根据这次抽样而犯第一类错误的最大概率，而无法知道具体在多大概率水平上犯错。计算P值有效的解决了这个问题，P值其实就是按照抽样分布计算的一个概率值，这个值是根据检验统计量计算出来的。通过直接比较P值与给定的显著性水平α的大小就可以知道是否拒绝原假设，显然这就可以代替比较检验统计量的具体值与临界值的大小的方法。***\n",
    "\n",
    "**<font color=#00BFFF>注意</font>**\n",
    "> - p值小于0.01---强有力的证据判定备择假设为真；\n",
    "> - p值介于0.01~0.05---有力的证据判定备择假设为真；\n",
    "> - p值介于0.05~0.1---较弱的证据判定备择假设为真；\n",
    "> - p值大于0.1---没有足够的证据判定备择假设为真。\n",
    "\n",
    "**<font color=#00BFFF>3.6 假设检验的种类</font>:$~$*Z检验、t检验、卡方检验、F检验。***\n",
    "> - <font color=#00BFFF>3.6.1 Z检验:$~$</font>Z检验用于比较样本和总体的均值是否不同或者两个样本的均值是否不同。检验统计量z值的分布服从正态分布,由于总体方差一般都是未知的，并且Z检验只适合大样本的情况。\n",
    "> - <font color=#00BFFF>3.6.2 T检验:$~$</font>事先不知道总体方差，另外，如果总体不服从正态分布，那么样本量要大于等于30，如果总体服从正态分布，那么对样本量没有要求.\n",
    "**<font color=#00BFFF>t检验又分为单样本t检验、配对t检验和独立样本t检验。</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.矩阵计算\n",
    "**<font color=#00BFFF>4.1 矩阵的加法与减法</font>:$~$**\n",
    "> - <font color=#00BFFF>4.1.1 运算规则:$~$</font>\n",
    ">设矩阵\n",
    "> $$A =\\begin{pmatrix}\n",
    "{a_{11}}&{a_{12}}&{\\cdots}&{a_{1n}}\\\\\n",
    "{a_{21}}&{a_{22}}&{\\cdots}&{a_{2n}}\\\\\n",
    "{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\\n",
    "{a_{m1}}&{a_{m2}}&{\\cdots}&{a_{mn}}\\\\\n",
    "\\end{pmatrix}$$\n",
    ">$$B =\\begin{pmatrix}\n",
    "{b_{11}}&{b_{12}}&{\\cdots}&{b_{1n}}\\\\\n",
    "{b_{21}}&{b_{22}}&{\\cdots}&{b_{2n}}\\\\\n",
    "{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\\n",
    "{b_{m1}}&{b_{m2}}&{\\cdots}&{b_{mn}}\\\\\n",
    "\\end{pmatrix}$$\n",
    "> 则\n",
    "> $$A\\pm B =\\begin{pmatrix}\n",
    "{a_{11}+b_{11}}&{a_{12}+b_{12}}&{\\cdots}&{a_{1n}+b_{1n}}\\\\\n",
    "{a_{21}+b_{21}}&{a_{22}+b_{21}}&{\\cdots}&{a_{2n}+b_{21}}\\\\\n",
    "{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\\n",
    "{a_{m1}+b_{m1}}&{a_{m2}+b_{m2}}&{\\cdots}&{a_{mn}+b_{mn}}\\\\\n",
    "\\end{pmatrix}$$\n",
    "> **简言之，两个矩阵相加减，即它们相同位置的元素相加减！**<br>\n",
    ">**<font color=#00BFFF>注意</font>:$~$ 只有对于两个行数、列数分别相等的矩阵（即同型矩阵），加减法运算才有意义，即加减运算是可行的．**\n",
    "> - <font color=#00BFFF>4.1.2 运算性质:$~$</font>\n",
    "> 满足交换律和结合律\n",
    "> - 交换律 $A+B=B+A$\n",
    "> - 结合律 $(A+B)+C=A+(B+C)$\n",
    "\n",
    "**<font color=#00BFFF>4.2 矩阵与数的乘法</font>:$~$**\n",
    "> - <font color=#00BFFF>4.2.1 运算规则:$~$</font><br>\n",
    "> 　$数\\lambda乘矩阵A，就是将数\\lambda乘矩阵A中的每一个元素，记为\\lambda A或A\\lambda．$<br>特别地，$称-A称A=(a_{ij})_{m \\times n}为的负矩阵．$\n",
    "> - <font color=#00BFFF>4.2.1 运算性质:$~$</font><br>\n",
    "> 满足结合律和分配律\n",
    "> - 分配律 $\\lambda(A+B)=\\lambda A+\\lambda B$\n",
    "> - 结合律 $(\\lambda\\mu)A=A(\\lambda\\mu); (\\lambda+\\mu)\\times A = \\lambda A+ \\mu A$\n",
    "\n",
    "**<font color=#00BFFF>4.3 矩阵与矩阵的乘法</font>:$~$**\n",
    "> - <font color=#00BFFF>4.3.1 运算规则:$~$</font>设\n",
    "> $$A=(a_{ij})_{m \\times n}~~~~B=(b_{ij})_{m \\times n}$$\n",
    "> 则A与B的乘积$C = AB是这样一个矩阵$:\n",
    "> - 行数与（左矩阵）A相同，列数与（右矩阵）B相同，即$$C=(c_{ij})_{m \\times n}$$\n",
    "> - $C的第i行第j列的元素由A的第i行元素与B的第j列元素对应相乘，再取乘积之和$\n",
    "> - <font color=#00BFFF>4.3.2 运算性质:$~$</font><br>\n",
    "> - 结合律 $(A+B)C=A(BC)$\n",
    "> - 分配律 $A(B\\pm C)=AB +AC; (B\\pm C)A=BA +CA$\n",
    "> - $(\\lambda A)B=\\lambda(AB)=A(\\lambda B)$\n",
    "> - <font color=#00BFFF>4.3.2 方阵的幂:$~$</font>设A是方阵，k是一个正整数，规定\n",
    "> $$A^0 =E,~~~A^K=\\underbrace{A·A···A}_{k}$$\n",
    "> 显然记号$A^k$表示A的连乘积.\n",
    "\n",
    "<font color=#00BFFF>4.4 矩阵的转置</font>:$~$**将矩阵A的行换成同序号的列所得到的新矩阵称为矩阵A的转置矩阵，记作$A^`或A^T.例如:$<br>**\n",
    "$$矩阵A=\\begin{bmatrix}\n",
    "1&0&3&1\\\\\n",
    "2&1&0&2\n",
    "\\end{bmatrix}$$\n",
    "转置矩阵为$$矩阵A=\\begin{bmatrix}\n",
    "1&2\\\\\n",
    "0&1\\\\\n",
    "3&0\\\\\n",
    "-1&2\n",
    "\\end{bmatrix}$$\n",
    "- <font color=#00BFFF>4.3.2 运算性质:$~$</font><br>\n",
    "> - $(A^T)^T=A$\n",
    "> - $(A+B)^T=A^T+B^T$\n",
    "> - $(AB)^T=B^TA^T$\n",
    "> - $(\\lambda A)^T=\\lambda A^T,~~\\lambda 是常数$\n",
    "\n",
    "<font color=#00BFFF>4.4 方阵的行列式</font>:$~~$:由方阵A的元素所构成的行列式（各元素的位置不变），称为方阵A的行列式，记作$|A|或det A$\n",
    "- <font color=#00BFFF>4.4.1 运算性质:$~$</font><br>\n",
    "> - $|A^T|=A$\n",
    "> - $|AB|^T=|A|·|B|.~特别:|A^k|=\\underbrace{|A||A|···|A|}_k$\n",
    "> - $|\\lambda A|=\\lambda^n|A|.~(\\lambda是常数,A的阶数为n)$\n",
    "> - $(\\lambda A)^T=\\lambda A^T,~~\\lambda 是常数$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.特征值及特征向量\n",
    "<font color=#00BFFF>5.1定义</font>:$~~$设A为n阶实方阵，如果存在某个数$\\lambda_0$及某个n维非零列向量$\\eta$，使得$A\\eta=\\lambda_0\\eta$，则称$\\lambda_0$是方阵A的一个特征值$\\lambda_0$是方阵A的一个特征值，$\\eta$是方阵A的属于特征值$\\lambda^0$的一个特征向量。\n",
    "\n",
    "**<font color=#00BFFF>注意</font>:**\n",
    "> 1. 定义说明特征值与特征向量是紧密相连的概念，对每个特征值必有属于它的特征向量，且有无穷多个；而对每个特征向量必属于某个特征值，且只属于一个特征值.\n",
    "> 2. 特征向量必须是非零向量，且必是列向量.\n",
    "> 3. 若$\\eta_1,\\eta_2$均为A的属于$\\eta_0$的特征向量，则对任何不全为零的数$k_1,k_2,\\eta = k_1\\eta_1+k_2\\eta_2$也是A的属于$\\lambda_0$的特征向量。\n",
    "\n",
    "<font color=#00BFFF>5.2特征根</font>:$~~$设A为n阶实方阵，$\\lambda$为一个参数，称n阶方阵$\\lambda E-A$为A的特征方阵，它的行列式$|\\lambda E-A|$称为A的特征多项式，把$|\\lambda E-A|=0$称为A的特征方程，把特征方程或特征多项式的根称为A的特征根。\n",
    "\n",
    "<font color=#00BFFF>5.3特征多项式</font>:$~~$设$A=(a_{ij})$为n阶实方阵,其特征多项式为 $$|\\lambda E-A|=\\begin{bmatrix}\n",
    "{\\lambda-a_{11}}&{-a_{12}}&{\\cdots}&{-a_{1n}}\\\\\n",
    "{-a_{21}}&{\\lambda-a_{22}}&{\\cdots}&{-a_{2n}}\\\\\n",
    "{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\\n",
    "{-a_{m1}}&{-a_{m2}}&{\\cdots}&{\\lambda-a_{mn}}\\\\\n",
    "\\end{bmatrix}$$\n",
    "$=\\lambda^n-(a_{11}+a_{22}+···+a_{nn})\\lambda^{n-1}+···+c_1\\lambda+c_0$<br>由行列式计算可知,行列式$|\\lambda E-A|$的值是$\\lambda$的n次多项式\n",
    "\n",
    "<font color=#00BFFF>5.4推导公步骤</font>:$~~$\n",
    ">1. 写出并计算A的特征多项式$|\\lambda E-A|$\n",
    ">2. 求特征方程$|\\lambda E-A|$=0的所有根，共有n个根（其中可能有复根，也可能有重根），这些根即为A的全部特征值；\n",
    ">3. 对于A的每一个特征值$\\lambda_i(1\\leq i\\leq n)$，求解齐次线性方程组$|\\lambda E-A|X=0$，则方程组的每一个非零解都是A的属于特征值$\\lambda_i$的特征向量（说明有无穷多个），特别方程组$|\\lambda E-A|X=0$的一个基础解系$\\lambda_{i_1},\\lambda_{i_2}···,\\lambda_{i_n}$就是方阵A的属于$\\lambda_i$的个数达最大的线性无关的特征向量，而且A的属于$\\lambda_i$的全部特征向量为$k_1\\lambda_{i_1},k_2\\lambda_{i_2}···,k_s\\lambda_{i_n}$，其中$k_1,k_2,···,k_s$是不全为零的任意常数。\n",
    "\n",
    ">例:设$A=\\begin{pmatrix}\n",
    "1&2\\\\\n",
    "2&4\n",
    "\\end{pmatrix},求A的所有特征值和特征向量.$ \n",
    "解:$$A的特征多项式|\\lambda E-A|=\\begin{vmatrix}\n",
    "\\lambda-1&-2\\\\\n",
    "-2&\\lambda-4\\\\\n",
    "\\end{vmatrix}=\\lambda(\\lambda-5)$$\n",
    "则A的两个特征值为$\\lambda_1=0,\\lambda_2=5$<br>\n",
    ">$$对\\lambda_1=0,求解齐次方程组(\\lambda E—A)X=0$$\n",
    ">$$0E-A = \\begin{pmatrix}\n",
    "-1&-2\\\\\n",
    "-2&-4\n",
    "\\end{pmatrix}\\to\\begin{pmatrix}\n",
    "1&2\\\\\n",
    "0&0\n",
    "\\end{pmatrix}$$\n",
    ">由此可求出方程组一个基础解系为$\\eta_1=\\begin{pmatrix}\n",
    "-1\\\\\n",
    "1\n",
    "\\end{pmatrix},则A的属于\\lambda_1=0的全部特征向量为k_1\\lambda_1(k_1)\\neq0$\n",
    "\n",
    ">$$对\\lambda_2=5,求解齐次方程组(\\lambda E—A)X=0$$\n",
    ">$$5E-A = \\begin{pmatrix}\n",
    "4&-2\\\\\n",
    "-2&1\n",
    "\\end{pmatrix}\\to\\begin{pmatrix}\n",
    "-2&1\\\\\n",
    "0&0\n",
    "\\end{pmatrix}$$\n",
    ">由此可求出方程组一个基础解系为$\\eta_2=\\begin{pmatrix}\n",
    "1\\\\\n",
    "2\n",
    "\\end{pmatrix},则A的属于\\lambda_2=5的全部特征向量为k_2\\lambda_2(k_2)\\neq0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.使用matplotlib绘图\n",
    "<font color=#00BFFF>说明</font>:$~~$ Matplotlib是一个python的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。通过Matplotlib，开发者可以仅需要几行代码，便可以生成绘图，直方图，功率谱，条形图，错误图，散点图等\n",
    "\n",
    "<font color=#00BFFF>6.1导包</font>:$~~$ **首先要导入包，在以后的示例中默认已经导入这两个包**\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "```\n",
    "<font color=#00BFFF>6.2画图</font>:$~~$ **然后画一个最基本的图**\n",
    "```python\n",
    "t = np.arange(0.0, 2.0, 0.01)#x轴上的点，0到2之间以0.01为间隔\n",
    "s = np.sin(2*np.pi*t)#y轴为正弦\n",
    "plt.plot(t, s)#画图\n",
    " \n",
    "plt.xlabel('time (s)')#x轴标签\n",
    "plt.ylabel('voltage (mV)')#y轴标签\n",
    "plt.title('About as simple as it gets, folks')#图的标签\n",
    "plt.grid(True)#产生网格\n",
    "plt.savefig(\"test.png\")#保存图像\n",
    "plt.show()#显示图像\n",
    "```\n",
    "![alt text](output1.png)\n",
    "\n",
    "<font color=#00BFFF>6.3定位</font>:$~~$ **这是在一个窗口中画单张图的过程，那么如何画多张图呢？画图的过程相同，无非是画多张，然后设定位置。**\n",
    "```python\n",
    "x1 = np.linspace(0.0, 5.0)#画图一\n",
    "x2 = np.linspace(0.0, 2.0)#画图二\n",
    "y1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n",
    "y2 = np.cos(2 * np.pi * x2)\n",
    " \n",
    "plt.subplot(2, 1, 1)#面板设置成2行1列，并取第一个（顺时针编号）\n",
    "plt.plot(x1, y1, 'yo-')#画图，染色\n",
    "plt.title('A tale of 2 subplots')\n",
    "plt.ylabel('Damped oscillation')\n",
    " \n",
    "plt.subplot(2, 1, 2)#面板设置成2行1列，并取第二个（顺时针编号）\n",
    "plt.plot(x2, y2, 'r.-')#画图，染色\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Undamped')\n",
    " \n",
    "plt.show()\n",
    "```\n",
    "![alt text](output2.png)\n",
    "\n",
    "<font color=#00BFFF>6.3直方图</font>:$~~$ **直方图的画法**\n",
    "```python\n",
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "mu = 100  # 正态分布的均值\n",
    "sigma = 15  # 标准差\n",
    "x = mu + sigma * np.random.randn(10000)#在均值周围产生符合正态分布的x值\n",
    " \n",
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "#直方图函数，x为x轴的值，normed=1表示为概率密度，即和为一，绿色方块，色深参数0.5.返回n个概率，直方块左边线的x值，及各个方块对象\n",
    "y = mlab.normpdf(bins, mu, sigma)#画一条逼近的曲线\n",
    "plt.plot(bins, y, 'r--')\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'Histogram of IQ: $\\mu=100$, $\\sigma=15$')#中文标题 u'xxx'\n",
    " \n",
    "plt.subplots_adjust(left=0.15)#左边距\n",
    "plt.show()\n",
    "```\n",
    "![alt text](output3.png)\n",
    "\n",
    "<font color=#00BFFF>6.4 3D图的画法</font>:$~~$ **3D离散点**\n",
    "```python\n",
    "# encoding: utf-8\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    " \n",
    "x_list = [[3,3,2],[4,3,1],[1,2,3],[1,1,2],[2,1,2]]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "for x in x_list:\n",
    "    ax.scatter(x[0],x[1],x[2],c='r')\n",
    "plt.show()\n",
    "```\n",
    "![alt text](output4.png)\n",
    "\n",
    "<font color=#00BFFF>6.5 3D图的画法</font>:$~~$ **画空间平面**\n",
    "```python\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "  \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "X=np.arange(1,10,1)\n",
    "Y=np.arange(1,10,1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = 3*X+2*Y+30\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,cmap=cm.jet,linewidth=0, antialiased=True)\n",
    "ax.set_zlim3d(0,100)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "```\n",
    "![alt text](output5.png)\n",
    "\n",
    "<font color=#00BFFF>6.5 画空间曲面</font>:$~~$ \n",
    "```python\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "X = np.arange(-5, 5, 0.1)\n",
    "Y = np.arange(-5, 5, 0.1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "R = np.sqrt(X**2 + Y**2)\n",
    "Z = np.sin(R)\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "#画表面,x,y,z坐标， 横向步长，纵向步长，颜色，线宽，是否渐变\n",
    "ax.set_zlim(-1.01, 1.01)#坐标系的下边界和上边界\n",
    " \n",
    "ax.zaxis.set_major_locator(LinearLocator(10))#设置Z轴标度\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))#Z轴精度\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)#shrink颜色条伸缩比例（0-1），aspect颜色条宽度（反比例，数值越大宽度越窄）\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "![alt text](output6.png)\n",
    "\n",
    "<font color=#00BFFF>6.5 饼图的画法</font>:$~~$ \n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'#设置标签\n",
    "sizes = [15, 30, 45, 10]#占比，和为100\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral']#颜色\n",
    "explode = (0, 0.1, 0, 0)  #展开第二个扇形，即Hogs，间距为0.1\n",
    " \n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,autopct='%1.1f%%', shadow=True, startangle=90)#startangle控制饼状图的旋转方向\n",
    "plt.axis('equal')#保证饼状图是正圆，否则会有一点角度偏斜\n",
    " \n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    " \n",
    "import numpy as np\n",
    " \n",
    "ax.pie(np.random.random(4), explode=explode, labels=labels, colors=colors,autopct='%1.1f%%', shadow=True, startangle=90, radius=0.25, center=(0, 0), frame=True)\n",
    "ax.pie(np.random.random(4), explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90, radius=0.25, center=(1, 1), frame=True)\n",
    "ax.pie(np.random.random(4), explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90, radius=0.25, center=(0, 1), frame=True)\n",
    "ax.pie(np.random.random(4), explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,radius=0.25, center=(1, 0), frame=True)\n",
    " \n",
    "ax.set_xticks([0, 1])#设置位置\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels([\"Sunny\", \"Cloudy\"])#设置标签\n",
    "ax.set_yticklabels([\"Dry\", \"Rainy\"])\n",
    "ax.set_xlim((-0.5, 1.5))\n",
    "ax.set_ylim((-0.5, 1.5))\n",
    " \n",
    "ax.set_aspect('equal')\n",
    "plt.show()\n",
    "```\n",
    "![alt text](output7.png)\n",
    "![alt text](output8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.主成因分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=#00BFFF>7.1 主成因分析</font>:*主成因分析（Principal Component 是使用最广的降维方法。PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。***\n",
    "\n",
    "<font color=#00BFFF>7.2 算法原理</font>:$~~$*主成因分析降维算法是一种非监督学习的降维方法。PCA算法利用特征值分解的思想和事件，将高维数据压缩或去噪成低维数据。\n",
    "> PCA算法由于某些固有缺陷，出现了很多PCA算法变种: \n",
    "> 1. 解决非线性降维的PCA方法、解决内存限制的增量PCA方法\n",
    "> 2. 解决稀疏数据降维的PCA方法。*\n",
    "\n",
    "\n",
    "<font color=#00BFFF>7.3 PCA降维算法的核心步骤</font>:$~~$\n",
    ">1. 对所有的样本进行中心化;\n",
    ">2. 计算样本的协方差矩阵$XX^T$ \n",
    ">3. 对协方差矩阵$XX^T$进行特征值分解\n",
    ">4. 取出最大的$n′$个特征值对应的特征向量$(w_1,w_2,…,w_n′)$, 计算标准化特征向量矩阵W;\n",
    ">5. 取出原始高维度样本$x_i$, 进行降维计算操作，获得低维度样本: $z_i=WTx_i$;\n",
    ">6. 输出降维后的样本集$ D′=(z_1,z_2,…,z_m) $;\n",
    "\n",
    "\n",
    "<font color=#00BFFF>7.4 PCA降维算法的核心优势</font>:$~~$\n",
    "> - **计算伸缩性:** 计算方法简单，主要运算是特征值分解，易于实现\n",
    "> - **参数依赖性**: 可配置目标维度数目n’;以方差衡量信息量，不受数据集以外的因素影响\n",
    "> - **普适性能力**: 特征值分解时各主成分之间正交，泛化能力较好\n",
    "> - **抗噪音能力**: 泛化能力较好，抗噪音能力强\n",
    "> - **结果解释性**: 特征维度的含义可能具有模糊性，解释性降低\n",
    "\n",
    "\n",
    "\n",
    "<font color=#00BFFF>7.5 算法实例</font>:$~~$ 使用sklearn中的鸢尾花数据集来对特征处理功能进行说明\n",
    "> 部分机器学习项目，由于数据集特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。\n",
    ">1. 常见的降维方法包括:基于L1惩罚项的模型、主成分分析法(PCA)和线性判别分析(LDA)。\n",
    ">2. 降维的本质就是将原始的样本映射到维度更低的样本空间中。\n",
    ">3. PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。\n",
    ">4. PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。\n",
    "\n",
    ">代码实现\n",
    "```python\n",
    "#\n",
    "# Python 3.9\n",
    "#\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(iris.keys())\n",
    "print(\"iris.keys()  = {0}\".format(iris.keys()))\n",
    "\n",
    "raw_data = iris['data']\n",
    "print(\"iris.data.shape  = {0}\".format(raw_data.shape))\n",
    "\n",
    "feature_names = iris['feature_names']\n",
    "print(\"data.feature_names  = {0}\".format(feature_names))\n",
    "\n",
    "raw_target = iris['target']\n",
    "print(\"data.target.shape = {0}\".format(raw_target.shape))\n",
    "\n",
    "def plot_iris_projection(x_index, y_index):\n",
    "    # plt.scatter one type of iris flower with one color.\n",
    "    types_count = 3\n",
    "    for t,marker,c in zip(range(types_count),'>ox', 'rgb'):\n",
    "        plt.scatter(dst_data[dst_target==t,x_index],\n",
    "                    dst_data[dst_target==t,y_index],\n",
    "                    marker=marker,c=c)\n",
    "    if(dst_data.shape[1] == 4):\n",
    "        plt.xlabel(feature_names[x_index])\n",
    "        plt.ylabel(feature_names[y_index])\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "dst_data   = raw_data\n",
    "dst_target = raw_target\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_iris_projection(x_index=0, y_index=1)\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_iris_projection(x_index=0, y_index=2)\n",
    "\n",
    "# unsupervised dimensionality reduction\n",
    "dst_data = PCA(n_components=2).fit_transform(raw_data)\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.set_title('PCA(n_components=2)')\n",
    "plot_iris_projection(x_index=0, y_index=1)\n",
    "print(dst_data.shape)\n",
    "\n",
    "# unsupervised dimensionality reduction\n",
    "dst_data = PCA(n_components=3).fit_transform(raw_data)\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.set_title('PCA(n_components=3)')\n",
    "plot_iris_projection(x_index=0, y_index=1)\n",
    "print(dst_data.shape)\n",
    "\n",
    "#Adjust subgraph spacing\n",
    "plt.subplots_adjust(wspace =0.3, hspace =0.4) \n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![alt text](output9.png)\n",
    "\n",
    "\n",
    "<font color=#00BFFF>7.6 总结</font>:$~~$由上图可以看出，sklearn.decomposition库的PCA类仅需要配置维度数量n_components即可，不需要额外的调参。原始数据维度print(raw_data.shape)为(150,4)。处理后为数据维度print(dst_data.shape)为(150, 2)和(150,3)得到了降维的目的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、机器学习及深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类问题\n",
    "\n",
    "**<font color=#00BFFF>1.分类算法优缺点:</font>**\n",
    "| 算法      | 优点\t | 缺点     |\n",
    "| :---        |    :----   |          :--- |\n",
    "| Bayes 贝叶斯分类法     | 1.所需估计的参数少，对于缺失数据不敏感<br>2.有着坚实的数学基础，以及稳定的分类效率       | 1.需要假设属性之间相互独立，这往往并不成立<br>2.需要知道先验概率<br>3.分类决策存在错误率|\n",
    "| Decision Tree决策树   | 1.不需要任何领域知识或参数假设。<br>2.适合高维数据。<br>3.简单易于理解<br>4.短时间内处理大量数据，得到可行且效果较好的结果<br>5.能够同时处理数据型和常规性属性。        | 1.对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征<br>2.易于过拟合<br>3.忽略属性之间的相关性<br>4.不支持在线学习。      |\n",
    "| SVM支持向量机   | 1.可以解决小样本下机器学习的问题<br>2.提高泛化性能<br>3.可以解决高维、非线性问题。超高维文本分类仍受欢迎<br>4.避免神经网络结构选择和局部极小的问题。        | 1.对缺失数据敏感<br>2.内存消耗大，难以解释<br>3.运行和调参略烦人。      |\n",
    "| KNN K近邻   | 1.思想简单，理论成熟，既可以用来做分类也可以用来做回归<br>2.可用于非线性分类<br>3.训练时间复杂度为O(n)<br>4.准确度高，对数据没有假设，对outlier不敏感        | 1.计算量太大<br>2.对于样本分类不均衡的问题，会产生误判<br>3.需要大量的内存<br>4.输出的可解释性不强。      |\n",
    "| Logistic Regression逻辑回归   | 1.速度快<br>2.简单易于理解，直接看到各个特征的权重<br>3.能容易地更新模型吸收新的数据<br>4.如果想要一个概率框架，动态调整分类阀值。        | 特征处理复杂。需要归一化和较多的特征工程      |\n",
    "| Neural Network 神经网络   | 1.分类准确率高<br>2.并行处理能力强<br>3.分布式存储和学习能力强<br>4.鲁棒性较强，不易受噪声影响。        | 1.需要大量参数（网络拓扑、阀值、阈值）<br>2.结果难以解释<br>3.训练时间过长。      |\n",
    "| Adaboosting   | 1.adaboost是一种有很高精度的分类器<br>2.可以使用各种方法构建子分类器，Adaboost算法提供的是框架<br>3.当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单<br>4.简单，不用做特征筛选<br>5.不用担心overfitting。        | 对outlier比较敏感      |\n",
    "\n",
    "**<font color=#00BFFF>2.常用术语:</font>**\n",
    "> 介绍几个常见的模型评价术语，现在假设我们的分类目标只有两类，计为正例（positive）和负例（negative）分别是\n",
    "> 1. **True positives(TP):** 被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数\n",
    ">2. **False positives(FP):** 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数\n",
    ">3. **False negatives(FN):** 被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数\n",
    ">4. **True negatives(TN):** 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数\n",
    "\n",
    ">| TP      | FN | P(实际是正例)     |\n",
    ">| :---:        |    :----:   |      :---: |\n",
    ">| FP      | TN       | F(实际为负例)   |\n",
    ">| P(划分为正例)   | F(划分为负例)        | P+N      |\n",
    "\n",
    "**<font color=#00BFFF>3.评价指标:</font>**\n",
    ">1. <font color=#00BFFF>正确率</font> :_正确率是我们最常见的评价指标，accuracy = (TP+TN)/(P+N)，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好_\n",
    ">2. <font color=#00BFFF>错误率</font>:*错误率则与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(P+N)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 - error rate*\n",
    ">3. <font color=#00BFFF>灵敏度</font>:*sensitivity = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。*\n",
    ">4. <font color=#00BFFF>特异性</font>:*specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。*\n",
    ">5. <font color=#00BFFF>精度查准率</font>:*precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。*\n",
    ">6. <font color=#00BFFF>召回率查全率</font>:*召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitivity，可以看到召回率与灵敏度是一样的。*\n",
    "\n",
    "**<font color=#00BFFF>4.ROC曲线和PR曲线:</font>**\n",
    "> - <font color=#00BFFF>ROC曲线:</font>ROC曲线是（Receiver Operating Characteristic Curve，受试者工作特征曲线）的简称，是以灵敏度（Sensitivity真阳性率）为纵坐标，以1减去特异性（Specificity假阳性率）为横坐标绘制的性能评价曲线。可以将不同模型对同一数据集的ROC曲线绘制在同一笛卡尔坐标系中，ROC曲线越靠近左上角，说明其对应模型越可靠。也可以通过ROC曲线下面的面积（Area Under Curve, AUC） 来评价模型，AUC越大，模型越可靠\n",
    "> - <font color=#00BFFF>PR曲线:</font>PR曲线是Precision Recall Curve的简称，描述的是precision查准率和recall查全率之间的关系，以recall为横坐标，precision为纵坐标绘制的曲线。该曲线的所对应的面积AUC实际上是目标检测中常用的评价指标平均精度（Average Precision, AP）。AP越高，说明模型性能越好。mAP 即 Mean Average Precision即平均AP值，是对多个验证集个体求平均AP值，作为 object dection中衡量检测精度的指标\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归问题\n",
    "\n",
    "**<font color=#00BFFF>1.回归问题:</font>** \n",
    "> - 回归问题也属于监督学习中的一类。回归用于预测输入变量与输出变量之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化\n",
    "> - 回归模型正是表示从输入变量到输出变量之间映射的函数。> - 回归问题按照输入变量的个数，可以分为一元回归和多元回归；按照输入变量与输出变量之间关系的类型，可以分为线性回归和非线性回归\n",
    "\n",
    "**<font color=#00BFFF>2.回归划分:</font>**\n",
    "> 1. 如果是连续的，就是多重线性回归\n",
    "> 2. 如果是二项分布，就是逻辑回归\n",
    "> 3. 如果是泊松分布，就是泊松回归\n",
    "> 4. 如果是负二项分布，就是负二项回归\n",
    "> 5. 逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的逻辑回归\n",
    "\n",
    "**<font color=#00BFFF>3.逻辑回归适用性:</font>**\n",
    "> 1. 用于概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大\n",
    ">2. 用于分类。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类\n",
    "> 3. 寻找危险因素。寻找某一疾病的危险因素等\n",
    ">4. 仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征\n",
    ">5. 各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算\n",
    "\n",
    "**<font color=#00BFFF>4.逻辑回归与朴素贝叶斯区别:</font>**\n",
    ">1. 逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有\n",
    ">2. 朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别\n",
    "> 3. 朴素贝叶斯需要条件独立假设\n",
    ">4. 逻辑回归需要求特征参数间是线性的\n",
    "\n",
    "**<font color=#00BFFF>5.线性回归与逻辑回归的区别:</font>**\n",
    "|             | 线性回归 | 逻辑回归     |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| 目的         | 预测       | 分类   |\n",
    "| $y^{(i)}$   | 未知        | （0,1）     |\n",
    "| 函数         | 拟合函数   | 预测函数     |\n",
    "| 参数计算方式  | 最小二乘法      | 极大似然估计    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积计算\n",
    "\n",
    "**<font color=#00BFFF>1.卷积计算:</font>** 输入有3个通道，输出有2个通道，没加激活函数\n",
    "![卷积计算](net1.png)\n",
    "\n",
    "*如上图所示，输入有3个通道，输出有2个通道，也就是有2个卷积核（一个卷积核就是322）。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量*\n",
    "> 1. <font color=#00BFFF>卷积核:</font>是一个参与运算的二维矩阵，对应位置相乘再相加。3个参数，大小、跨度、padding边缘填充\n",
    "> 2. <font color=#00BFFF>卷积运算原理:</font>最基本的就是单通道对应一个输出通道，那就用一个二维矩阵（比如33）对输入图像进行运算，提取图像特征，原理如下图。但是单通道输入得到多通道（k个通道）输出，就需要k个滤波器，每个滤波器是就是一个二维的33矩阵，也就是每个滤波器包含一个卷积核。但是当输入是多通道（t）得到多通道（k个通道）输出，那这次卷积就包括k个滤波器，每个滤波器就是一个3维矩阵，33t，每个滤波器就包含t个卷积核。上面输出通道是k,那就得到了k个特征图，每个输出通道就是一个特征图。\n",
    "![卷积计算](net1.gif)\n",
    "![卷积计算](net2.gif)\n",
    ">3. 输入是一个5x5x3的矩阵，有三个通道。filter是一个3x3x3的矩阵。首先，filter中的每个卷积核（记住卷积核是3维的，不是2维的）分别应用于输入层的三个通道。执行三次卷积，产生3个3x3的通道\n",
    ">4. 这三个通道相加（矩阵加法），得到一个3x3x1的单通道。这个通道就是在输入层(5x5x3矩阵）应用filter(3x3x3矩阵）的结构\n",
    "![卷积计算](net3.gif)\n",
    ">5. filter和kernel之间的不同很微妙。很多时候，它们可以互换，所以这可能造成我们的混淆。那它们之间的不同在于哪里呢？一个\"kernel\"更倾向于是2D的权重矩阵。而’filter\"则是指多个Kernel堆叠的3D结构。如果是一个2D的filter，那么两者就是一样的。但是一个3Dfilter, 在大多数深度学习的卷积中，它是包含kernel的。每个卷积核都是独一无二的，主要在于强调输入通道的不同方面\n",
    "\n",
    "\n",
    "**<font color=#00BFFF>2.卷积:</font>**\n",
    "> <font color=#00BFFF>2.1目的:</font>\n",
    "> - 卷积运算的目的是提取输入的不同特征,某些卷积层可能只能提取一些低级的特征如边缘、线条等层级,更多层的网络能从低级特征迭代提取更复杂的特征. \n",
    "\n",
    "> <font color=#00BFFF>2.2参数:</font>\n",
    "> - size卷积核/过滤器大小\n",
    "> - padding: 零填充,Valid与Same\n",
    "> - stride步长,通常默认为1\n",
    "\n",
    "> <font color=#00BFFF>2.3计算公式:</font>\n",
    "> - 输入体积大小$H_1\\times W_1\\times D_1$\n",
    ">- 四个超参数<br>1. Filter数量K<br>2.Filter大小F<br>3.步长S<br>4.零填充大小P\n",
    ">- 输出体积大小$H_2\\times W_2\\times D_2$<br>- $H_2=(H_1-F+2P)/S+1$<br>- $W_2=(W_1-F+2P)/S+1$<br>-$D_2=K$\n",
    "\n",
    "**<font color=#00BFFF>3.填充:</font>**\n",
    ">- <font color=#00BFFF>Padding-零填充:</font>在图片像素的最外层加上若干层0值,若一层,记做$p=1$\n",
    ">- <font color=#00BFFF>Valid and Same卷积:</font><br>-valid不填充也就是最终大小为<br>$(N-F+1)\\times (N-F+1)$<br>-Same输出大小与原图大小一致,那么$N$就变成$N+2P$<br>$(N+2P-F+1)\\times (N+2P-F+1)$\n",
    "\n",
    ">意味着之前大小与之后大小一样得出:\n",
    ">$$(N+2P-F+1)=N$$ \n",
    ">$$P=\\frac{F-1}{2}$$ \n",
    ">所以当知道卷积核大小之后,就可以得出需要填充多少层像素\n",
    "\n",
    "**<font color=#00BFFF>4.步长:</font>**\n",
    "\n",
    "![卷积计算](net2.png)\n",
    "> <font color=#00BFFF>stride-步长:</font>将步长设置为2、3,如果一原来的计算公式那么结果$N+2P-F+1 = 6+0-3+1=4$<br>但是移动两个像素才得出一个结果,所以公式变为$$\\frac{N+2P-F}{2} +1=1.5+1=2.5$$如果相除不是整数的时候向下取整,故为2.<br>\n",
    "><font color=#00BFFF>最终公式:</font>对于输入图片大小为N,过滤器大小为F,步长为S,零填充为P $$(\\frac{N+2P-F}{S}+1)\\times ~(\\frac{N+2P-F}{S}+1)$$\n",
    "\n",
    "**<font color=#00BFFF>5. 多通道卷积和多卷积核:</font>**\n",
    ">- <font color=#00BFFF>多通道卷积:</font>当输入有多个通道时.卷积核需要拥有相同的channel数,每个卷积核channel与输入层的对应channel进行卷积,将每个channel的卷积结果按位相加得到最终的结果\n",
    "\n",
    "![卷积计算](net3.png)\n",
    ">- <font color=#00BFFF>卷积多核:</font>当有多个卷积核时,可以学习到多种不同的特征,对应产生包含多个channel的FeatureMap.\n",
    "\n",
    "![卷积计算](net4.png)\n",
    "\n",
    "**<font color=#00BFFF>6.设计卷积:</font>**\n",
    ">- <font color=#00BFFF>单个卷积计算公式:</font>\n",
    "\n",
    "![卷积计算](net5.png)\n",
    "\n",
    "**<font color=#00BFFF>7. 代码实现:</font>**\n",
    "\n",
    "![卷积计算](net7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def conv_(img, conv_filter):\n",
    "    \"\"\"\n",
    "    卷积核计算操作\n",
    "    :param img: 图片数据\n",
    "    :param conv_filter: 卷积核\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 1、获取卷积核的大小\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    # 初始化卷积后的结果，给个较大的输出结果\n",
    "    result = np.zeros((img.shape))\n",
    "    # 2、对图片进行循环使用卷积操作（获取当前区域并使用过滤器进行相乘操作.）\n",
    "    # （1）r和c为特征图的下表，从0到特征图输出大小\n",
    "    for r in np.uint16(np.arange(filter_size/2.0, img.shape[0]-filter_size/2.0+1)):\n",
    "        for c in np.uint16(np.arange(filter_size/2.0, img.shape[1]-filter_size/2.0+1)):\n",
    "            # 取出过滤器大小的图片区域，从图片左上角开始\n",
    "            curr_region = img[r-np.uint16(np.floor(filter_size/2.0)):r+np.uint16(np.ceil(filter_size/2.0)),\n",
    "                              c-np.uint16(np.floor(filter_size/2.0)):c+np.uint16(np.ceil(filter_size/2.0))]\n",
    "            # 图片当前区域与卷积核进行线性相乘\n",
    "            curr_result = curr_region * conv_filter\n",
    "            # 结果求和并保存，按照下表保存\n",
    "            conv_sum = np.sum(curr_result)\n",
    "            result[r, c] = conv_sum\n",
    "\n",
    "    # 裁剪矩阵\n",
    "    final_result = result[np.uint16(filter_size/2.0):result.shape[0]-np.uint16(filter_size/2.0),\n",
    "                          np.uint16(filter_size/2.0):result.shape[1]-np.uint16(filter_size/2.0)]\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def conv(img, conv_filter):\n",
    "    \"\"\"\n",
    "    卷积过程实现\n",
    "    :param img: 图像\n",
    "    :param conv_filter: 卷积过滤器\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 1、输入的参数大小做异常检测\n",
    "    # 检查输入的图片和卷积核是否一样大小\n",
    "    if len(img.shape) != len(conv_filter.shape) - 1:\n",
    "        print(\"Error: Number of dimensions in conv filter and image do not match.\")\n",
    "        exit()\n",
    "    # 检查输入的图片的通道数和卷积的深度一样\n",
    "    if len(img.shape) > 2 or len(conv_filter.shape) > 3:\n",
    "        if img.shape[-1] != conv_filter.shape[-1]:\n",
    "            print(\"Error: Number of channels in both image and filter must match.\")\n",
    "            sys.exit()\n",
    "    # 检查是否过滤器的长宽一样\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]:\n",
    "        print('Error: Filter must be a square matrix. I.e. number of rows and columns must match.')\n",
    "        sys.exit()\n",
    "    # 检查过滤器的维度是奇数\n",
    "    if conv_filter.shape[1] % 2 == 0:\n",
    "        print('Error: Filter must have an odd size. I.e. number of rows and columns must be odd.')\n",
    "        sys.exit()\n",
    "\n",
    "    # 2、初始化一个空的特征图来装入计算的结果\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filter.shape[1]+1,\n",
    "                                img.shape[1]-conv_filter.shape[1]+1,\n",
    "                                conv_filter.shape[0]))\n",
    "\n",
    "    # 3、图片的卷积完整操作(分别使用每一个过滤器进行过滤操作)\n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "        print(\"Filter \", filter_num + 1)\n",
    "        # 获取当前的filter参数\n",
    "        curr_filter = conv_filter[filter_num, :]\n",
    "\n",
    "        # 当前filter进行卷积核计算操作\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            # 对图片的每个channel进行卷积运算\n",
    "            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0])\n",
    "            for ch_num in range(1, curr_filter.shape[-1]):\n",
    "                conv_map = conv_map + conv_(img[:, :, ch_num], curr_filter[:, :, ch_num])\n",
    "        else:\n",
    "            # 只有一个filter的情况\n",
    "            conv_map = conv_(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map\n",
    "    return feature_maps\n",
    "\n",
    "# 使用过程\n",
    "# 1、定义这层有两个卷积核，每个大小3x3(例子默认对黑白图片进行计算)，默认一个步长，不零填充\n",
    "l1_filter = np.zeros((2,3,3))\n",
    "# 初始化参数\n",
    "l1_filter[0, :, :] = np.array([[[-1, 0, 1], \n",
    "                                   [-1, 0, 1], \n",
    "                                   [-1, 0, 1]]])\n",
    "l1_filter[1, :, :] = np.array([[[1,   1,  1], \n",
    "                                   [0,   0,  0], \n",
    "                                   [-1, -1, -1]]])\n",
    "# 卷积计算\n",
    "l1_feature_map = cnn.conv(img, l1_filter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度神经网络\n",
    "**<font color=#00BFFF>1.DNN模型:</font>** 深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础\n",
    ">- <font color=#00BFFF>DNN网络结构:</font>DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层，如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层\n",
    "![卷积计算](net8.png)\n",
    "> *一般说到神经网络的层数是这样计算的，输入层不算，从隐藏层开始一直到输出层，一共有几层就代表着这是一个几层的神经网络*\n",
    "\n",
    "**<font color=#00BFFF>2.正向传播:</font>** 正向传播(forward-propagation)是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量(包括输出\n",
    "![卷积计算](net9.png)\n",
    "**<font color=#00BFFF>3.反向传播:</font>** 反向传播(back-propagation)指的是计算神经网络参数梯度的方法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。\n",
    "> - 正向传播求损失，BP回传误差\n",
    "> - 根据误差信号修正每层的权重。对各个w进行求导，然后更新各个w\n",
    "> - 链式依赖损失函数:$y^-=h(g(f(x)))$\n",
    "\n",
    "**<font color=#00BFFF>4.神经网络是端到端的网络:</font>**\n",
    ">- 端到端学习(end-to-end)是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果\n",
    ">- 就是不要预处理和特征提取，直接把原始数据扔进去得到最终结果\n",
    ">- 特征提取包含在神经网络内部，所以说神经网络是端到端的网络\n",
    ">- <font color=#00BFFF>优点:</font>通过缩减人工预处理和后续处理，尽可能使模型从原始输入到最终输出，给模型更多可以根据数据自动调节的空间，增加模型的整体契合度\n",
    ">- <font color=#00BFFF>缺点:</font><br>1.它可能需要大量的数据。要直接学到这个𝑥到𝑦的映射，你可能需要大量(𝑥, 𝑦)数据<br>2.它排除了可能有用的手工设计组件"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0233bebd06b1c8dfbe4378b0a68f7ba42066a2023ae2305a086f654849648f0f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('PaperClassify')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
